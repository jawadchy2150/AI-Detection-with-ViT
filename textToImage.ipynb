{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae68470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Jawad/AI detection with ViT/vit_text_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "911ba463",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PER_CLASS = 5000\n",
    "MAX_CHAR = 3500\n",
    "OUTPUT_DIR = \"triplet_output\"\n",
    "\n",
    "IMAGE_SIZE = (1024, 1024)\n",
    "FONT_SIZE = 14\n",
    "MARGIN = 15\n",
    "LINE_SPACING = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72969332",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_FONT = \"/System/Library/Fonts/Times New Roman.ttf\" \n",
    "LATEX_FONT = \"/System/Library/Fonts/Supplemental/Times.ttc\"  \n",
    "BROWSER_FONT = \"/System/Library/Fonts/Supplemental/Arial.ttf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769667a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_text_to_image(text, font_path, output_path):\n",
    "    try:\n",
    "        font = ImageFont.truetype(font_path, FONT_SIZE)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    img = Image.new(\"RGB\", IMAGE_SIZE, \"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    max_width = IMAGE_SIZE[0] - 2 * MARGIN\n",
    "    avg_char_width = font.getlength(\"A\")\n",
    "    max_chars_per_line = int(max_width / avg_char_width)\n",
    "\n",
    "    wrapped_text = textwrap.fill(text, width=max_chars_per_line)\n",
    "\n",
    "    draw.multiline_text(\n",
    "        (MARGIN, MARGIN),\n",
    "        wrapped_text,\n",
    "        fill=\"black\",\n",
    "        font=font,\n",
    "        spacing=LINE_SPACING\n",
    "    )\n",
    "\n",
    "    img.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e60416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "AI samples: 5000 → 15000 images\n",
      "Human samples: 5000 → 15000 images\n",
      "Total images: 30000\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ds = load_dataset(\"artem9k/ai-text-detection-pile\", split=\"train\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    ai_count = 0\n",
    "    human_count = 0\n",
    "\n",
    "    for row in ds:\n",
    "        text = row[\"text\"]\n",
    "        text_id = row[\"id\"]\n",
    "        source = row[\"source\"].lower()\n",
    "\n",
    "        if len(text) > MAX_CHAR:\n",
    "            continue\n",
    "\n",
    "        if source == \"ai\" and ai_count < MAX_PER_CLASS:\n",
    "            label_ok = True\n",
    "        elif source == \"human\" and human_count < MAX_PER_CLASS:\n",
    "            label_ok = True\n",
    "        else:\n",
    "            label_ok = False\n",
    "\n",
    "        if not label_ok:\n",
    "            continue\n",
    "\n",
    "        render_text_to_image(text, DOC_FONT, f\"{OUTPUT_DIR}/{text_id}_doc.png\")\n",
    "        render_text_to_image(text, LATEX_FONT, f\"{OUTPUT_DIR}/{text_id}_latex.png\")\n",
    "        render_text_to_image(text, BROWSER_FONT, f\"{OUTPUT_DIR}/{text_id}_browser.png\")\n",
    "\n",
    "        if source == \"ai\":\n",
    "            ai_count += 1\n",
    "        else:\n",
    "            human_count += 1\n",
    "\n",
    "        if ai_count == MAX_PER_CLASS and human_count == MAX_PER_CLASS:\n",
    "            break\n",
    "\n",
    "    print(\"======================================\")\n",
    "    print(f\"AI samples: {ai_count} → {ai_count * 3} images\")\n",
    "    print(f\"Human samples: {human_count} → {human_count * 3} images\")\n",
    "    print(f\"Total images: {(ai_count + human_count) * 3}\")\n",
    "    print(\"======================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bd3789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    15000\n",
      "1    15000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('metadata.csv')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aeb2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc = df[df['variant'] == 'doc'].copy()\n",
    "df_doc.to_csv('metadata_exp1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd44fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images of doc: 10000\n",
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"total images of doc:\", len(df_doc))\n",
    "print(df_doc['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cd6319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in ./vit_text_env/lib/python3.12/site-packages (from scikit-learn) (2.3.5)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbd2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('metadata_exp1.csv')\n",
    "unique_ids = df['text_id'].unique()\n",
    "\n",
    "train_ids, temp_ids = train_test_split(unique_ids, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "df['split'] = \"train\"\n",
    "df.loc[df['text_id'].isin(val_ids), \"split\"] = \"val\"\n",
    "df.loc[df['text_id'].isin(test_ids), \"split\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "986d6da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    7000\n",
      "test     1500\n",
      "val      1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['split'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945ecd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"metadata_exp1_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8bed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in ./vit_text_env/lib/python3.12/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./vit_text_env/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./vit_text_env/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Using cached torch-2.9.1-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, MarkupSafe, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [torch]32m6/7\u001b[0m [torch]kx]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 setuptools-80.9.0 sympy-1.14.0 torch-2.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d606fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b38c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_image_dataset import TextImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b06def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    csv_file=\"metadata_exp1_split.csv\",\n",
    "    image_dir=\"triplet_output\",\n",
    "    batch_size=32\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit_text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
